# ğŸš€ AI-Dev System - Ready for Safe Deployment

Your AI-Dev system is **production-ready** and configured for **safe incremental deployment** to your K3s cluster.

## âœ… What's Been Built

### Complete System (29 Files)

**Kubernetes Manifests** (15 files):
- âœ… Namespace and storage (PVCs)
- âœ… Qdrant vector database
- âœ… vLLM inference server with GPU time-slicing
- âœ… Code indexer (CronJob + manual job)
- âœ… SWE-agent deployments and jobs
- âœ… Traefik ingress with auth and rate limiting

**Scripts** (5 files):
- âœ… `deploy-safe.sh` - Interactive incremental deployment
- âœ… `deploy.sh` - Original full deployment
- âœ… `validate-manifests.sh` - Pre-deployment validation
- âœ… `test-vllm-api.py` - Comprehensive API testing
- âœ… `check-plex-health.sh` - GPU conflict detection

**Python Code** (2 files):
- âœ… `index_code.py` - 350+ line code indexer
- âœ… Test suite in test-vllm-api.py

**Documentation** (7 files, 72KB):
- âœ… `SAFE_DEPLOYMENT_GUIDE.md` - **START HERE** (27KB)
- âœ… `GPU_TIMESLICING.md` - Complete GPU configuration (11KB)
- âœ… `GPU_SETUP_SUMMARY.md` - What was changed for GPU (7KB)
- âœ… `DEPLOYMENT_QUICK_REF.md` - Quick reference card (5KB)
- âœ… `README.md` - Full system documentation (16KB)
- âœ… `QUICKSTART.md` - 30-minute guide (7KB)
- âœ… `DEPLOYMENT_CHECKLIST.md` - Step-by-step validation (9KB)
- âœ… `PROJECT_SUMMARY.md` - Architecture overview (12KB)

**Other**:
- âœ… Dockerfile for code-indexer
- âœ… ConfigMaps and Secrets templates
- âœ… Kustomization file for GitOps

## ğŸ¯ GPU Configuration Verified

Your vLLM deployment is configured to match your existing GPU time-slicing setup:

### Hybrid GPU Access (Like Ollama)
```yaml
# Webhook triggers automatic device mounting
annotations:
  gpu-directories: nvidia

# Explicit runtime class
runtimeClassName: nvidia

# Time-sliced resource allocation (2 shares = 25% GPU time)
resources:
  limits:
    nvidia.com/gpu.shared: "2"
  requests:
    nvidia.com/gpu.shared: "2"

# NVIDIA environment variables
env:
- name: NVIDIA_VISIBLE_DEVICES
  value: "all"
- name: NVIDIA_DRIVER_CAPABILITIES
  value: "compute,utility"

# Conservative memory limit (70% = ~8.4GB of 12GB)
- name: GPU_MEMORY_UTILIZATION
  value: "0.70"
```

### GPU Share Allocation
| Workload | Before | After |
|----------|--------|-------|
| Ollama | 1 | 1 |
| TTS | 1 | 1 |
| Whisper | 2 | 2 |
| **vLLM** | **-** | **2** â† NEW |
| Available | 4 | 2 |

## ğŸ“‹ Pre-Deployment Tasks (You Need to Do)

### 1. Build Code Indexer Image (Required)

```bash
cd ai-dev/code-indexer

# Build
docker build -t code-indexer:latest .

# Option A: Push to registry
docker tag code-indexer:latest your-registry.io/code-indexer:latest
docker push your-registry.io/code-indexer:latest

# Then update cronjob.yaml with your image path

# Option B: Use local (if cluster can access Docker daemon)
# No push needed, update cronjob.yaml to: code-indexer:latest
```

### 2. Configure Your Repositories (Required)

Edit `code-indexer/configmap.yaml`:
```yaml
repositories:
  - name: "grok-servaar"
    url: "https://github.com/yourusername/grok-servaar.git"
  - name: "your-app"
    url: "https://github.com/yourusername/your-app.git"
  # Add all repos you want indexed
```

### 3. Create GitHub Token (For SWE-agent, Optional)

1. Go to: https://github.com/settings/tokens/new
2. Scopes: `repo`, `workflow`
3. Generate token
4. Save temporarily: `echo "ghp_YourToken" > /tmp/github-token`

### 4. Configure API Auth (Optional)

Default credentials: `user:password` (CHANGE THIS!)

To generate new credentials:
```bash
# Generate password hash
htpasswd -nb admin yourpassword | base64

# Update ai-dev/ingress/ingressroute.yaml
# Replace data.users with your hash
```

### 5. Update Domain (Optional)

Edit `ingress/ingressroute.yaml`:
```yaml
match: Host(`your-domain.com`)  # Change from code-llm.archer.casa
```

## ğŸš€ Safe Deployment Options

### Option 1: Interactive Incremental Deployment (RECOMMENDED)

**Best for**: First deployment, production clusters, when you want control

```bash
# Run from repository root
bash ai-dev/scripts/deploy-safe.sh
```

**What it does**:
- âœ… Deploys in 6 phases with validation gates
- âœ… Pauses between phases for your approval
- âœ… Checks Plex health after GPU deployment
- âœ… Provides clear status and next steps
- âœ… Easy to abort at any time

**Time**: 20-30 minutes (including model download)

### Option 2: Automated Deployment (For Experts)

**Best for**: Second deployment, testing, when you're confident

```bash
# Non-interactive mode
INTERACTIVE=0 bash ai-dev/scripts/deploy-safe.sh
```

**Warning**: Skips validation gates! Only use if you know what you're doing.

### Option 3: Manual Phase-by-Phase

**Best for**: Maximum control, debugging issues

Follow: `DEPLOYMENT_QUICK_REF.md` or `SAFE_DEPLOYMENT_GUIDE.md`

Deploy each phase manually with validation between steps.

### Option 4: Kustomize (GitOps)

**Best for**: Integrating with existing GitOps workflow

```bash
kubectl apply -k ai-dev/
```

**Note**: No validation gates, all components deploy at once.

## ğŸ“– Deployment Documentation

| Document | Purpose | When to Read |
|----------|---------|--------------|
| **SAFE_DEPLOYMENT_GUIDE.md** | Complete deployment strategy | **Read this first!** |
| **DEPLOYMENT_QUICK_REF.md** | Quick reference card | Print and keep handy |
| **GPU_TIMESLICING.md** | GPU configuration details | Before GPU deployment |
| **README.md** | Full system documentation | After deployment |
| **QUICKSTART.md** | Condensed guide | Alternative to safe guide |

## âš ï¸ Critical Safety Rules

1. **Plex is Priority #1**
   - If Plex breaks, rollback immediately
   - Always check Plex after GPU deployment
   - Use `check-plex-health.sh` script

2. **Deploy During Low Usage**
   - Early morning (2-6 AM)
   - When Plex transcoding is minimal
   - Have someone test Plex during deployment

3. **Monitor GPU Memory**
   - First hour after deployment
   - Watch for OOM errors
   - Adjust if vLLM > 10GB VRAM

4. **Easy Rollback Available**
   - Quick: `kubectl delete deployment -n ai-dev vllm-server`
   - Full: `kubectl delete namespace ai-dev`
   - Always verify Plex recovers

5. **Validation Gates Matter**
   - Don't skip validation checks
   - If something fails, fix before continuing
   - Each phase builds on previous

## ğŸ¯ Success Criteria

After deployment, you should have:

- âœ… All pods in `ai-dev` namespace Running
- âœ… vLLM responding to API calls
- âœ… GPU visible in vLLM pod (`nvidia-smi` works)
- âœ… **Plex pod still Running**
- âœ… **Plex transcoding still works**
- âœ… Qdrant has code embeddings
- âœ… Can query from Cline/Claude Code
- âœ… GPU shows 6/8 shares used
- âœ… GPU memory ~8-10GB (vLLM)
- âœ… No CUDA OOM errors

## ğŸš¨ If Something Goes Wrong

### Quick Emergency Actions

**Plex Broken**:
```bash
kubectl delete deployment -n ai-dev vllm-server
sleep 30
bash ai-dev/scripts/check-plex-health.sh
```

**vLLM Won't Start**:
```bash
kubectl describe pod -n ai-dev -l app=vllm
kubectl logs -n ai-dev -l app=vllm
# Check events and logs for clues
```

**GPU OOM**:
```bash
kubectl edit configmap -n ai-dev vllm-config
# Set GPU_MEMORY_UTILIZATION: "0.60"
kubectl rollout restart deployment -n ai-dev vllm-server
```

**Full Rollback**:
```bash
kubectl delete namespace ai-dev
bash ai-dev/scripts/check-plex-health.sh
```

## ğŸ“ Getting Help

**During Deployment**:
1. Check `SAFE_DEPLOYMENT_GUIDE.md` troubleshooting section
2. Review pod logs: `kubectl logs -n ai-dev -l app=<component>`
3. Describe resources: `kubectl describe pod -n ai-dev <pod>`
4. Check events: `kubectl get events -n ai-dev`

**After Deployment**:
1. See `README.md` for usage and configuration
2. Check `GPU_TIMESLICING.md` for GPU tuning
3. Review component logs for errors

## ğŸ“ Recommended Deployment Flow

**Day Before**:
1. âœ… Read `SAFE_DEPLOYMENT_GUIDE.md` completely
2. âœ… Build code-indexer Docker image
3. âœ… Configure repositories in configmap
4. âœ… Create GitHub token (if using SWE-agent)
5. âœ… Print `DEPLOYMENT_QUICK_REF.md`
6. âœ… Schedule deployment time (low Plex usage)

**Deployment Day**:
1. âœ… Open `DEPLOYMENT_QUICK_REF.md` in terminal
2. âœ… Run `bash ai-dev/scripts/deploy-safe.sh`
3. âœ… Follow prompts, validate each phase
4. âœ… **Watch Plex health after Phase 3**
5. âœ… Run final validation tests
6. âœ… Configure IDE (Cline/Claude Code)

**After Deployment**:
1. âœ… Monitor for 24 hours
2. âœ… Test Plex transcoding multiple times
3. âœ… Watch GPU memory trends
4. âœ… Test AI-dev API from IDE
5. âœ… Document any issues/adjustments

## ğŸ”„ What Happens During Deployment

### Timeline (Estimated)

- **Phase 1**: Namespace + Storage â†’ 1 minute
- **Phase 2**: Qdrant â†’ 2 minutes
- **Phase 3**: vLLM (GPU) â†’ **10-15 minutes** (model download)
- **Phase 4**: Code Indexer â†’ 1 minute
- **Phase 5**: SWE-agent â†’ 1 minute (optional)
- **Phase 6**: Ingress â†’ 1 minute (optional)
- **Validation**: 5 minutes

**Total**: ~20-30 minutes

### What Takes Time

- â±ï¸ Model download (5-10 minutes) - first time only
- â±ï¸ PVC provisioning (1-2 minutes)
- â±ï¸ Image pulls (2-5 minutes) - first time
- â±ï¸ Health checks (readiness probes)

### What to Watch

- Terminal output from `deploy-safe.sh`
- Plex web UI (have it open)
- GPU usage: `watch kubectl exec -n ai-dev -l app=vllm -- nvidia-smi`
- Pod status: `watch kubectl get pods -n ai-dev`

## ğŸ“¦ Next Steps After Successful Deployment

1. **Test the API**
   ```bash
   kubectl port-forward -n ai-dev svc/vllm-server 8000:8000 &
   python3 ai-dev/scripts/test-vllm-api.py
   ```

2. **Index Your Code**
   ```bash
   kubectl create job --from=cronjob/code-indexer manual-index -n ai-dev
   kubectl logs -n ai-dev -l app=code-indexer -f
   ```

3. **Configure Cline**
   - API URL: `http://localhost:8000/v1`
   - Model: `deepseek-coder-6.7b-instruct`
   - Test with simple query

4. **Monitor for 24 Hours**
   - Check Plex transcoding works
   - Watch GPU memory usage
   - Review vLLM logs for errors
   - Test AI coding assistance

5. **Optional: Fine-Tune**
   - Collect training data
   - Train LoRA adapter
   - Deploy to adapter-storage PVC

## ğŸ‰ You're Ready!

Everything is prepared for safe deployment. The system is:

- âœ… GPU time-slicing compatible
- âœ… Plex-aware and safe
- âœ… Incrementally deployable
- âœ… Easy to rollback
- âœ… Comprehensively documented
- âœ… Production-ready

**Start Here**:
```bash
bash ai-dev/scripts/deploy-safe.sh
```

Good luck! ğŸš€

---

**Questions?** Check the documentation:
- **Deployment**: `SAFE_DEPLOYMENT_GUIDE.md`
- **GPU**: `GPU_TIMESLICING.md`
- **Usage**: `README.md`
- **Quick Ref**: `DEPLOYMENT_QUICK_REF.md`
